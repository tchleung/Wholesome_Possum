{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from cleaner import clean_text\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define handy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics2(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all_comments_with_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelaunch Focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_start = '2020-03-13' #inclusive\n",
    "period_stop = '2020-03-20' #exclusive\n",
    "\n",
    "df_pre_pos = df[(df['date'] < period_stop) & (df['date'] >= period_start) & (df['sentiment'] == 'pos')]\n",
    "df_pre_neu = df[(df['date'] < period_stop) & (df['date'] >= period_start) & (df['sentiment'] == 'neu')]\n",
    "df_pre_neg = df[(df['date'] < period_stop) & (df['date'] >= period_start) & (df['sentiment'] == 'neg')]\n",
    "df_pre_pos.drop(columns=['Unnamed: 0','id','subreddit'],inplace=True)\n",
    "df_pre_neu.drop(columns=['Unnamed: 0','id','subreddit'],inplace=True)\n",
    "df_pre_neg.drop(columns=['Unnamed: 0','id','subreddit'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_pre_pos.loc[:,'cleaned'] = df_pre_pos['body'].apply(clean_text)\n",
    "df_pre_pos.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_pre_neu.loc[:,'cleaned'] = df_pre_neu['body'].apply(clean_text)\n",
    "df_pre_neu.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_pre_neg.loc[:,'cleaned'] = df_pre_neg['body'].apply(clean_text)\n",
    "df_pre_neg.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>sent_score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-13 14:00:21</td>\n",
       "      <td>soundwave145</td>\n",
       "      <td>Doom,Final fantasy 7,Resident evil 3,Persona 5...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>pos</td>\n",
       "      <td>doom final fantasy resident evil persona royal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020-03-13 14:00:22</td>\n",
       "      <td>frescapades</td>\n",
       "      <td>Dang! Just went to pick one up and bc there wa...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>pos</td>\n",
       "      <td>dang went pick one bc barcode website anything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020-03-13 14:00:43</td>\n",
       "      <td>pearlescentsheep</td>\n",
       "      <td>SW-1421-4584-3913 â€” Probably going with Pearl...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>pos</td>\n",
       "      <td>probably going pearl still deciding island nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2020-03-13 14:00:45</td>\n",
       "      <td>derekscardigan</td>\n",
       "      <td>The adding and removing insurance tip was key....</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>pos</td>\n",
       "      <td>adding removing insurance tip key thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2020-03-13 14:00:58</td>\n",
       "      <td>AnonymousSplash</td>\n",
       "      <td>That sounds adorable! I think we have similar ...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0.9095</td>\n",
       "      <td>pos</td>\n",
       "      <td>sound adorable think similar mindset trying de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93720</td>\n",
       "      <td>2020-03-19 23:59:46</td>\n",
       "      <td>Smiles-Bite</td>\n",
       "      <td>It isn't great in the least. Our virus count i...</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>pos</td>\n",
       "      <td>great least virus count skyrocketing still pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93722</td>\n",
       "      <td>2020-03-19 23:59:47</td>\n",
       "      <td>Flux85</td>\n",
       "      <td>You do realize a major part of the game in thi...</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>pos</td>\n",
       "      <td>realize major part game version resides server...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93723</td>\n",
       "      <td>2020-03-19 23:59:54</td>\n",
       "      <td>Benson2500</td>\n",
       "      <td>Of course you would. I 100% believe you lmao</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>pos</td>\n",
       "      <td>course would believe lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93724</td>\n",
       "      <td>2020-03-19 23:59:54</td>\n",
       "      <td>BloomyC</td>\n",
       "      <td>Really cute! I would buy from you.</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0.5551</td>\n",
       "      <td>pos</td>\n",
       "      <td>really cute would buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93725</td>\n",
       "      <td>2020-03-19 23:59:54</td>\n",
       "      <td>cairowednesday</td>\n",
       "      <td>Thank you!</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>pos</td>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50590 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime            author  \\\n",
       "1      2020-03-13 14:00:21      soundwave145   \n",
       "2      2020-03-13 14:00:22       frescapades   \n",
       "4      2020-03-13 14:00:43  pearlescentsheep   \n",
       "5      2020-03-13 14:00:45    derekscardigan   \n",
       "8      2020-03-13 14:00:58   AnonymousSplash   \n",
       "...                    ...               ...   \n",
       "93720  2020-03-19 23:59:46       Smiles-Bite   \n",
       "93722  2020-03-19 23:59:47            Flux85   \n",
       "93723  2020-03-19 23:59:54        Benson2500   \n",
       "93724  2020-03-19 23:59:54           BloomyC   \n",
       "93725  2020-03-19 23:59:54    cairowednesday   \n",
       "\n",
       "                                                    body        date  \\\n",
       "1      Doom,Final fantasy 7,Resident evil 3,Persona 5...  2020-03-13   \n",
       "2      Dang! Just went to pick one up and bc there wa...  2020-03-13   \n",
       "4       SW-1421-4584-3913 â€” Probably going with Pearl...  2020-03-13   \n",
       "5      The adding and removing insurance tip was key....  2020-03-13   \n",
       "8      That sounds adorable! I think we have similar ...  2020-03-13   \n",
       "...                                                  ...         ...   \n",
       "93720  It isn't great in the least. Our virus count i...  2020-03-19   \n",
       "93722  You do realize a major part of the game in thi...  2020-03-19   \n",
       "93723       Of course you would. I 100% believe you lmao  2020-03-19   \n",
       "93724                 Really cute! I would buy from you.  2020-03-19   \n",
       "93725                                         Thank you!  2020-03-19   \n",
       "\n",
       "       sent_score sentiment                                            cleaned  \n",
       "1          0.0772       pos  doom final fantasy resident evil persona royal...  \n",
       "2          0.2003       pos  dang went pick one bc barcode website anything...  \n",
       "4          0.1007       pos  probably going pearl still deciding island nam...  \n",
       "5          0.4199       pos            adding removing insurance tip key thank  \n",
       "8          0.9095       pos  sound adorable think similar mindset trying de...  \n",
       "...           ...       ...                                                ...  \n",
       "93720      0.3235       pos  great least virus count skyrocketing still pro...  \n",
       "93722      0.1154       pos  realize major part game version resides server...  \n",
       "93723      0.5994       pos                          course would believe lmao  \n",
       "93724      0.5551       pos                              really cute would buy  \n",
       "93725      0.4199       pos                                              thank  \n",
       "\n",
       "[50590 rows x 7 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataframes\n",
    "df_pre_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(\n",
    "#     max_df = 0.99,\n",
    "#     min_df = 0.01,\n",
    "#     max_features = \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = tfidfvectorizer.fit_transform(df_pre_neg['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10670, 10782)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer(\n",
    "#     max_df=0.95,\n",
    "#     min_df=2,\n",
    "#     max_features=n_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = countvectorizer.fit_transform(df_pre_neg['cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 10\n",
    "random_seed = 99\n",
    "ldamodel = LatentDirichletAllocation(\n",
    "    n_components=number_of_topics,\n",
    "    max_iter=50,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=50.0,\n",
       "                          max_doc_update_iter=100, max_iter=50,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=99, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.fit(count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "      <th>Topic 6 words</th>\n",
       "      <th>Topic 6 weights</th>\n",
       "      <th>Topic 7 words</th>\n",
       "      <th>Topic 7 weights</th>\n",
       "      <th>Topic 8 words</th>\n",
       "      <th>Topic 8 weights</th>\n",
       "      <th>Topic 9 words</th>\n",
       "      <th>Topic 9 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>get</td>\n",
       "      <td>684.6</td>\n",
       "      <td>oh</td>\n",
       "      <td>245.5</td>\n",
       "      <td>played</td>\n",
       "      <td>140.3</td>\n",
       "      <td>im</td>\n",
       "      <td>175.4</td>\n",
       "      <td>save</td>\n",
       "      <td>73.6</td>\n",
       "      <td>digital</td>\n",
       "      <td>411.4</td>\n",
       "      <td>game</td>\n",
       "      <td>1030.8</td>\n",
       "      <td>damn</td>\n",
       "      <td>170.6</td>\n",
       "      <td>animal</td>\n",
       "      <td>494.1</td>\n",
       "      <td>die</td>\n",
       "      <td>210.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>game</td>\n",
       "      <td>655.5</td>\n",
       "      <td>hate</td>\n",
       "      <td>189.4</td>\n",
       "      <td>never</td>\n",
       "      <td>105.8</td>\n",
       "      <td>love</td>\n",
       "      <td>129.6</td>\n",
       "      <td>as</td>\n",
       "      <td>57.7</td>\n",
       "      <td>copy</td>\n",
       "      <td>389.3</td>\n",
       "      <td>one</td>\n",
       "      <td>835.4</td>\n",
       "      <td>like</td>\n",
       "      <td>104.8</td>\n",
       "      <td>crossing</td>\n",
       "      <td>450.9</td>\n",
       "      <td>sorry</td>\n",
       "      <td>180.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>itch</td>\n",
       "      <td>492.1</td>\n",
       "      <td>pick</td>\n",
       "      <td>187.3</td>\n",
       "      <td>moved</td>\n",
       "      <td>103.3</td>\n",
       "      <td>fruit</td>\n",
       "      <td>95.5</td>\n",
       "      <td>meant</td>\n",
       "      <td>50.6</td>\n",
       "      <td>physical</td>\n",
       "      <td>251.4</td>\n",
       "      <td>like</td>\n",
       "      <td>647.4</td>\n",
       "      <td>look</td>\n",
       "      <td>56.1</td>\n",
       "      <td>post</td>\n",
       "      <td>214.8</td>\n",
       "      <td>man</td>\n",
       "      <td>123.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>order</td>\n",
       "      <td>462.2</td>\n",
       "      <td>worry</td>\n",
       "      <td>104.0</td>\n",
       "      <td>villager</td>\n",
       "      <td>97.9</td>\n",
       "      <td>bob</td>\n",
       "      <td>72.6</td>\n",
       "      <td>true</td>\n",
       "      <td>50.0</td>\n",
       "      <td>two</td>\n",
       "      <td>152.0</td>\n",
       "      <td>time</td>\n",
       "      <td>636.3</td>\n",
       "      <td>cant</td>\n",
       "      <td>51.4</td>\n",
       "      <td>shit</td>\n",
       "      <td>179.3</td>\n",
       "      <td>fuck</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>store</td>\n",
       "      <td>447.1</td>\n",
       "      <td>dont</td>\n",
       "      <td>99.8</td>\n",
       "      <td>name</td>\n",
       "      <td>82.2</td>\n",
       "      <td>sadly</td>\n",
       "      <td>70.5</td>\n",
       "      <td>axe</td>\n",
       "      <td>43.7</td>\n",
       "      <td>bad</td>\n",
       "      <td>136.6</td>\n",
       "      <td>new</td>\n",
       "      <td>581.0</td>\n",
       "      <td>eye</td>\n",
       "      <td>50.5</td>\n",
       "      <td>doom</td>\n",
       "      <td>120.7</td>\n",
       "      <td>op</td>\n",
       "      <td>55.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>got</td>\n",
       "      <td>392.2</td>\n",
       "      <td>suck</td>\n",
       "      <td>66.0</td>\n",
       "      <td>read</td>\n",
       "      <td>80.7</td>\n",
       "      <td>lazy</td>\n",
       "      <td>67.3</td>\n",
       "      <td>japanese</td>\n",
       "      <td>40.0</td>\n",
       "      <td>want</td>\n",
       "      <td>91.0</td>\n",
       "      <td>people</td>\n",
       "      <td>561.3</td>\n",
       "      <td>color</td>\n",
       "      <td>50.2</td>\n",
       "      <td>miss</td>\n",
       "      <td>95.1</td>\n",
       "      <td>ich</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>day</td>\n",
       "      <td>365.9</td>\n",
       "      <td>yet</td>\n",
       "      <td>62.1</td>\n",
       "      <td>code</td>\n",
       "      <td>76.2</td>\n",
       "      <td>always</td>\n",
       "      <td>66.8</td>\n",
       "      <td>favourite</td>\n",
       "      <td>34.6</td>\n",
       "      <td>scared</td>\n",
       "      <td>79.4</td>\n",
       "      <td>get</td>\n",
       "      <td>558.9</td>\n",
       "      <td>design</td>\n",
       "      <td>49.7</td>\n",
       "      <td>see</td>\n",
       "      <td>72.6</td>\n",
       "      <td>actual</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>release</td>\n",
       "      <td>362.1</td>\n",
       "      <td>essential</td>\n",
       "      <td>60.5</td>\n",
       "      <td>question</td>\n",
       "      <td>66.9</td>\n",
       "      <td>sure</td>\n",
       "      <td>66.2</td>\n",
       "      <td>dizzy</td>\n",
       "      <td>34.1</td>\n",
       "      <td>problem</td>\n",
       "      <td>76.5</td>\n",
       "      <td>would</td>\n",
       "      <td>493.7</td>\n",
       "      <td>idk</td>\n",
       "      <td>47.1</td>\n",
       "      <td>please</td>\n",
       "      <td>58.0</td>\n",
       "      <td>da</td>\n",
       "      <td>34.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>still</td>\n",
       "      <td>344.9</td>\n",
       "      <td>keep</td>\n",
       "      <td>60.1</td>\n",
       "      <td>cute</td>\n",
       "      <td>57.7</td>\n",
       "      <td>hard</td>\n",
       "      <td>65.1</td>\n",
       "      <td>rock</td>\n",
       "      <td>32.5</td>\n",
       "      <td>really</td>\n",
       "      <td>71.9</td>\n",
       "      <td>know</td>\n",
       "      <td>474.5</td>\n",
       "      <td>send</td>\n",
       "      <td>46.6</td>\n",
       "      <td>information</td>\n",
       "      <td>56.0</td>\n",
       "      <td>image</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>mine</td>\n",
       "      <td>316.0</td>\n",
       "      <td>non</td>\n",
       "      <td>54.9</td>\n",
       "      <td>house</td>\n",
       "      <td>57.0</td>\n",
       "      <td>mad</td>\n",
       "      <td>64.1</td>\n",
       "      <td>personality</td>\n",
       "      <td>31.6</td>\n",
       "      <td>gyroids</td>\n",
       "      <td>69.0</td>\n",
       "      <td>really</td>\n",
       "      <td>469.4</td>\n",
       "      <td>bitch</td>\n",
       "      <td>46.6</td>\n",
       "      <td>removed</td>\n",
       "      <td>48.7</td>\n",
       "      <td>cherry</td>\n",
       "      <td>33.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 0 words Topic 0 weights Topic 1 words Topic 1 weights Topic 2 words  \\\n",
       "0           get           684.6            oh           245.5        played   \n",
       "1          game           655.5          hate           189.4         never   \n",
       "2          itch           492.1          pick           187.3         moved   \n",
       "3         order           462.2         worry           104.0      villager   \n",
       "4         store           447.1          dont            99.8          name   \n",
       "5           got           392.2          suck            66.0          read   \n",
       "6           day           365.9           yet            62.1          code   \n",
       "7       release           362.1     essential            60.5      question   \n",
       "8         still           344.9          keep            60.1          cute   \n",
       "9          mine           316.0           non            54.9         house   \n",
       "\n",
       "  Topic 2 weights Topic 3 words Topic 3 weights Topic 4 words Topic 4 weights  \\\n",
       "0           140.3            im           175.4          save            73.6   \n",
       "1           105.8          love           129.6            as            57.7   \n",
       "2           103.3         fruit            95.5         meant            50.6   \n",
       "3            97.9           bob            72.6          true            50.0   \n",
       "4            82.2         sadly            70.5           axe            43.7   \n",
       "5            80.7          lazy            67.3      japanese            40.0   \n",
       "6            76.2        always            66.8     favourite            34.6   \n",
       "7            66.9          sure            66.2         dizzy            34.1   \n",
       "8            57.7          hard            65.1          rock            32.5   \n",
       "9            57.0           mad            64.1   personality            31.6   \n",
       "\n",
       "  Topic 5 words Topic 5 weights Topic 6 words Topic 6 weights Topic 7 words  \\\n",
       "0       digital           411.4          game          1030.8          damn   \n",
       "1          copy           389.3           one           835.4          like   \n",
       "2      physical           251.4          like           647.4          look   \n",
       "3           two           152.0          time           636.3          cant   \n",
       "4           bad           136.6           new           581.0           eye   \n",
       "5          want            91.0        people           561.3         color   \n",
       "6        scared            79.4           get           558.9        design   \n",
       "7       problem            76.5         would           493.7           idk   \n",
       "8        really            71.9          know           474.5          send   \n",
       "9       gyroids            69.0        really           469.4         bitch   \n",
       "\n",
       "  Topic 7 weights Topic 8 words Topic 8 weights Topic 9 words Topic 9 weights  \n",
       "0           170.6        animal           494.1           die           210.6  \n",
       "1           104.8      crossing           450.9         sorry           180.4  \n",
       "2            56.1          post           214.8           man           123.4  \n",
       "3            51.4          shit           179.3          fuck           103.5  \n",
       "4            50.5          doom           120.7            op            55.7  \n",
       "5            50.2          miss            95.1           ich            37.8  \n",
       "6            49.7           see            72.6        actual            37.8  \n",
       "7            47.1        please            58.0            da            34.1  \n",
       "8            46.6   information            56.0         image            33.9  \n",
       "9            46.6       removed            48.7        cherry            33.6  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "ct_feature_names = countvectorizer.get_feature_names()\n",
    "show_topics(ldamodel,ct_feature_names,top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "p = pyLDAvis.sklearn.prepare(ldamodel, count_vec, countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(p, 'lda_pre_neg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=2000,\n",
       "    n_components=10, random_state=99, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 10\n",
    "random_seed = 99\n",
    "nmfmodel = NMF(\n",
    "    n_components=number_of_topics,\n",
    "    max_iter=2000,\n",
    "    random_state=random_seed)\n",
    "nmfmodel.fit(tf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: get time like people really bad know want think way\n",
      "Topic #1: sorry im man hear mean suck omg right meant sold\n",
      "Topic #2: mine got th still say delayed ordered amazon yet shipped\n",
      "Topic #3: problem thank yeah enjoy bud lt see fun need make\n",
      "Topic #4: oh damn shit god yeah hell suck well hot sad\n",
      "Topic #5: game animal crossing played doom never play stop first release\n",
      "Topic #6: new sad villager leaf town horizon moved favorite first away\n",
      "Topic #7: one itch island ac name per hacked console hard know\n",
      "Topic #8: digital cancel copy order physical buy pre go get cancelled\n",
      "Topic #9: would die cry ride goldie doubt slider marshal hard pay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "tf_feature_names = tfidfvectorizer.get_feature_names()\n",
    "show_topics2(nmfmodel,tf_feature_names,top_n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "period_start = '2020-03-20' #inclusive\n",
    "period_stop = '2020-04-01' #exclusive\n",
    "\n",
    "df_launch = df[(df['date'] < period_stop) & (df['date'] >= period_start)]\n",
    "df_launch.drop(columns=['Unnamed: 0','id','subreddit'],inplace=True)\n",
    "\n",
    "df_launch.loc[:,'cleaned'] = df_launch['body'].apply(clean_text)\n",
    "df_launch.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_launch.to_csv('../data/launch_cleaned_all_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer_launch = TfidfVectorizer(\n",
    "#     max_df = 0.99,\n",
    "#     min_df = 0.01,\n",
    "#     max_features = \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238242, 43316)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_focus = 'pos'\n",
    "filtered_frame = df_launch[df_launch['sentiment'] == sentiment_focus]['cleaned']\n",
    "\n",
    "tf_vec_launch = tfidfvectorizer_launch.fit_transform(filtered_frame)\n",
    "tf_vec_launch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=2000,\n",
       "    n_components=10, random_state=99, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 10\n",
    "random_seed = 99\n",
    "nmfmodel_launch = NMF(\n",
    "    n_components=number_of_topics,\n",
    "    max_iter=2000,\n",
    "    random_state=random_seed)\n",
    "nmfmodel_launch.fit(tf_vec_launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: thank much awesome oh okay ok ah great amazing know\n",
      "Topic #1: code dodo please looking fruit dm peach orange cherry apple\n",
      "Topic #2: thanks ok much awesome oh okay cool know info ah\n",
      "Topic #3: yes please omg move day believe ah one unfortunately oh\n",
      "Topic #4: like island get one game time day villager know want\n",
      "Topic #5: love would much visit omg come amazing see great design\n",
      "Topic #6: lol oh yeah got ok mine omg thought know need\n",
      "Topic #7: nice look oh really job would wow great work good\n",
      "Topic #8: friend code request sent add play best send new itch\n",
      "Topic #9: cute awesome super omg look really oh great idea job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "tf_feature_names_launch = tfidfvectorizer_launch.get_feature_names()\n",
    "show_topics2(nmfmodel_launch,tf_feature_names_launch,top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163143, 30733)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_focus = 'neu'\n",
    "filtered_frame = df_launch[df_launch['sentiment'] == sentiment_focus]['cleaned']\n",
    "\n",
    "tf_vec_launch = tfidfvectorizer_launch.fit_transform(filtered_frame)\n",
    "tf_vec_launch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=2000,\n",
       "    n_components=10, random_state=99, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 10\n",
    "random_seed = 99\n",
    "nmfmodel_launch = NMF(\n",
    "    n_components=number_of_topics,\n",
    "    max_iter=2000,\n",
    "    random_state=random_seed)\n",
    "nmfmodel_launch.fit(tf_vec_launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: code dodo dm qr pm send looking open new hemisphere\n",
      "Topic #1: island come fruit open visit nook hemisphere fish southern mile\n",
      "Topic #2: added open gate name back pop ya bring hi add\n",
      "Topic #3: cherry peach pear apple orange trade bring looking coconut fruit\n",
      "Topic #4: get recipe nook mile able game villager house trying ladder\n",
      "Topic #5: got today mine pear balloon recipe one first yesterday iron\n",
      "Topic #6: way make know game go think find headed work bringing\n",
      "Topic #7: day one remindme time think next first villager new game\n",
      "Topic #8: sent request dm pm message chat amp link check downloadable\n",
      "Topic #9: need still fruit iron know open anything omg shop space\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "tf_feature_names_launch = tfidfvectorizer_launch.get_feature_names()\n",
    "show_topics2(nmfmodel_launch,tf_feature_names_launch,top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73427, 23125)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_focus = 'neg'\n",
    "filtered_frame = df_launch[df_launch['sentiment'] == sentiment_focus]['cleaned']\n",
    "\n",
    "tf_vec_launch = tfidfvectorizer_launch.fit_transform(filtered_frame)\n",
    "tf_vec_launch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=2000,\n",
       "    n_components=10, random_state=99, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 10\n",
    "random_seed = 99\n",
    "nmfmodel_launch = NMF(\n",
    "    n_components=number_of_topics,\n",
    "    max_iter=2000,\n",
    "    random_state=random_seed)\n",
    "nmfmodel_launch.fit(tf_vec_launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: time game like people think day thing get really know\n",
      "Topic #1: sorry loss know sure im new closed mean hear internet\n",
      "Topic #2: problem lol know fun else exact anyone yeah see fix\n",
      "Topic #3: island tarantula mystery nook mile fruit ticket native bamboo flower\n",
      "Topic #4: tree get axe fruit rock shake shovel hit stone flimsy\n",
      "Topic #5: bad want feel luck lol really like go make bot\n",
      "Topic #6: oh damn shit god fuck know thanks thank hell suck\n",
      "Topic #7: code dodo drop need peach cherry orange apple fruit pear\n",
      "Topic #8: one got day caught first fish per catch bait two\n",
      "Topic #9: villager move first house get plot campsite leave nook invite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 10\n",
    "tf_feature_names_launch = tfidfvectorizer_launch.get_feature_names()\n",
    "show_topics2(nmfmodel_launch,tf_feature_names_launch,top_n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bunny day focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/tl/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/home/tl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "period_start = '2020-04-01' #inclusive\n",
    "period_stop = '2020-04-13' #exclusive\n",
    "\n",
    "df_bunny = df[(df['date'] < period_stop) & (df['date'] >= period_start)]\n",
    "df_bunny.drop(columns=['Unnamed: 0','id','subreddit'],inplace=True)\n",
    "\n",
    "df_bunny.loc[:,'cleaned'] = df_bunny['body'].apply(clean_text)\n",
    "df_bunny.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bunny.to_csv('../data/bunny_cleaned_all_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF - Bunny day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer_bunny = TfidfVectorizer(\n",
    "#     max_df = 0.99,\n",
    "#     min_df = 0.01,\n",
    "#     max_features = \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151479, 27319)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_focus = 'neu'\n",
    "filtered_frame = df_bunny[df_bunny['sentiment'] == sentiment_focus]['cleaned']\n",
    "\n",
    "tf_vec_bunny = tfidfvectorizer_bunny.fit_transform(filtered_frame)\n",
    "tf_vec_bunny.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=2000,\n",
       "    n_components=20, random_state=99, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 20\n",
    "random_seed = 99\n",
    "nmfmodel_bunny = NMF(\n",
    "    n_components=number_of_topics,\n",
    "    max_iter=2000,\n",
    "    random_state=random_seed)\n",
    "nmfmodel_bunny.fit(tf_vec_bunny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: recipe cherry blossom diy balloon extra wand trade petal craft give bonsai sakura lantern flooring wall make anyone branch bunny\n",
      "Topic #1: dm looking ed check dodo selling anyone hey price make sending celeste let turnip tip offer send sent cant trade\n",
      "Topic #2: code dodo send qr creator give new message bring design work open able sister hey post shop use visit working\n",
      "Topic #3: get recipe star rid trying wait able way balloon could point item back even net many isabelle rock trophy wallpaper\n",
      "Topic #4: island mystery visit tarantula go flick spawn find bamboo bug someone found people fish rock fruit name hemisphere celeste native\n",
      "Topic #5: one make first per could found seen another new give two craft balloon find buy thought also gotten different would\n",
      "Topic #6: need many craft omg anything star space much really iron life crafted make item extra material thing still least flower\n",
      "Topic #7: day bunny next remindme th last every event first wait per april zipper item two ago believe arch back show\n",
      "Topic #8: pm ed dodo amp sunday link looking around downloadable price send bot spawn turnip leaf sending fish right check tip\n",
      "Topic #9: got today balloon mine yesterday star first bottle finally recipe hour peach never game two yet last night ago morning\n",
      "Topic #10: nook sell turnip mile bell buy ticket price cranny shop looking tom selling item able anyone give tip go upgrade\n",
      "Topic #11: come visit back mine could wanna hey may buy sell th let hi tomorrow town trying bring shop catalog sunday\n",
      "Topic #12: villager move house campsite plot new random first give amiibo way wait someone talk ask moving moved land kick already\n",
      "Topic #13: time travel back go every first game traveling turnip traveled take backwards spoil rot people work change thing see event\n",
      "Topic #14: fruit looking orange peach rose pear apple red bring flower native trade seed white yellow mum shop cosmos cherry tree\n",
      "Topic #15: sent message request chat amp link downloadable mom via also work code video info boop beep provides github redd support\n",
      "Topic #16: still open shop gate able available plot looking game hey sister going visitor store hi work hour people even take\n",
      "Topic #17: egg fish water make bait balloon craft sky many use catch tree spawn bug earth zipper give sell getting bunny\n",
      "Topic #18: know let anyone right game oh make could even thing way far really work item much would see mean find\n",
      "Topic #19: think game work would right thing random way april balloon dont might th tree people mean around mine even say\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_words = 20\n",
    "tf_feature_names_bunny = tfidfvectorizer_bunny.get_feature_names()\n",
    "show_topics2(nmfmodel_bunny,tf_feature_names_bunny,top_n_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
